%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CHANGE ME}

\begin{document} 

\twocolumn[
\icmltitle{Exploiting the Hidden Information in the Labels}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Sunsern Cheamanunkul}{scheaman@eng.ucsd.edu}
\icmladdress{UCSD,
            9500 Gilman Dr., La Jolla, CA 92093}
\icmlauthor{Yoav Freund}{yfreund@eng.ucsd.edu}
\icmladdress{UCSD,
            9500 Gilman Dr., La Jolla, CA 92093}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract} 
TODO
\end{abstract} 

\section{Introduction}
\label{sec:intro}

Consider a supervised classification problem where there are $M$
classes and $n$ training examples per class. In $k$-NN, the label of a
novel test example is predicted by the majority of labels of the
training examples in the neighborhood of size $k$. Fix and Hodges
shows that, asypmtotically, the optimal error rate can be achieved by choosing a large
enough $k$ but small compared to $n$. Additionaly, Cover and Hart
shows that, when $k = 1$, the error rate $r$ is upperbounded by $r^*(2
- \frac{M}{M-1}r^*)$. Nonetheless, in non-asymptotic scenarios, there
is no guarantee of how the k-NN will perform.

We propose a new inference method for the k-NN algorithm which is
potentially applicable to other algorithm as well. The proposed method
involves considering the entire distribution of labels in the
neighborhood instead of just the majority. While the majority rule
works in most cases, it completely ignores the non-maximum classes which, in some case, can
contain crucial information. For example, consider the following
hypothetical scenario. Suppose the data of class 1 and of class 3
often get confused with each other. For some reason, the data of class
1 never have the data of class 2 in their neighborhood, but this is
not the case for the data of class 3. We are given a novel point whose
neighborhood indicates that this point belongs to either class 1 or
class 3 with class 1 being the majority. Additionally, the data from
class 2 are also observed in the neighborhood. The classic $k$-NN
predicts the majority class which class 1. However, we have the
additional information about the presence of class 2 in the
neighborhood which suggests otherwise.

We present results from both synthetic data and real datasets. We
obtain a hugh improvement on uRight dataset and a small improvement on
MNIST and SVHN dataset.

\section{Related Work}
\label{sec:related}

Work that uses label structure

* The closest to this work is ``Learning label embeddings for
nearest-neighbor multi-class classification with an application to
speech recognition'' by Nathsha Singh-Miller and Michael Collins. In
that paper, they proposed a method for learning label embeddings based
on ECOCs. The difference is how the label embeddings is learned and
used.

* Error-correcting output codes by Dietterich and Bakiri. Another
related paper is ``reducing multiclass to binary: A unify approach for
Margin classifiers'' by Allwien, Schapire, Singer.

Samy Bengio's work on label embedding tree.

Work that suggest alternatives to likelihood-ratio test.

* Intransitive likelihood-ratio classifiers by Bilmes, Ji and Meila.


\section{Background}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\S}{\mathcal{S}}
\newcommand{\nh}{\mathcal{N}}

Let $\S = \{ (x_1,y_1,) \ldots (x_n,y_n)\}$ be a set of training
examples where each instance $x_i$ belongs to a domain $\X$ with a
metric $d(\cdot,\cdot)$. Without loss of generality, we assume that
each label $y_i$ takes on a value from $\{1,2,\ldots,m\}$. 

Let $\nh_k(x)$ denote the neighborhood of size $k$ of an example $x
\in \X$ with respect to the metric $d$. The traditional $k$-NN
algorithm predicts the label of an example $x$ with the majority of
the labels in $nh_k(x)$. When $n \rightarrow \infty$ and $k$ is large
but, at the same time, small with respect to $n$, this decision rule is
in fact optimal~\cite{Fix and Hodges}. However, there is no such
guarantee for finite $n$.

Suppose the true class label $y$ of an example $(x,y)$ is drawn from a fixed
but unknown distribution $p$ whose support is the set of all possible
labels. Using the $k$-NN algorithm, we can estimate $p$ with an
empirical distribution $P$ defined by
\[
P(y|x) = \frac{\#\{ \mbox{occurrences of label } y \mbox{ in } \nh_k(x)\}}{k}
\]. 
It is easy to see that the majority vote rule is equivalent to the
maximum likelihood estimate of the true label assuming the posterior
distribution is given by $P(y|x)$. Specifically, the predicted label
$\hat{y}$ is given by
\[
\hat{y} = \arg\max_y P(y|x)
\]

Under some conditions, the majority rule is clearly sub-optimal
because the prediction is made based entirely on the maximal value of
the distribution $P$. Sometimes, the information about the
true label can be found in the minority of the neighborhood. For
example, suppose we know that the examples from class A never appears
in the neighborhood of examples from class B and examples from class B
and class C are often confusable. Given a test example $x$ that we
want to identify its class, we estimate $P(B | x) = 0.45$ and $P(C |
x) = 0.4$. However, we observe a number of examples from class A in
the neightborhood of $x$. The majority rule will predict class B
disregarding the additional information that we have and lead to a
wrong prediction.

To address this problem, we would like to modify the $k$-NN algorithm
so that it takes into account the entire distribution of $P$ instead
of just the maximum coordinate of $P$. One way to achieve this is to
measure the distance from $P$ to each of the prototypical
distributions of each class and to predict with the class whose
distance is minimized. Let $Q_j$ denote the prototypical distribution
for class label $j$. We can think of $Q_j$ as a distribution that
describes the probability that an example from class $j$ get confused
with other classes. A natural distance measure of two distributions is
KL-divergence. Our algorithm is summaried in Algorithm 1.

Under certain assumptions, it can be shown that using $KL$ divegence
is the right thing to do. For any example $(x,y)$, suppose that the class label $y$ is
generated by the following two-step process. First, $j$ is drawn
uniformly from the set $\{1,2,...,m\}$. Then, the class label $y$ is
then drawn from a fixed but unknown distribution $Q_j$. With these
assumptions, we can show that the maximum likelihood estimate of the class
label is the same as the class label estimated by minimizng the KL
divergence.
  
\newcommand{\dkl}{D_{\mathrm{KL}}}

We define the following shorthand notations:
\begin{enumerate}
\item Let $y^k = [y_1, \ldots, y_k]$ denote a sample of size $k$ drawn
  IID from an unknown distribution whose support is $\Y$
\item Let $P_{y^k}$ denote a distribution over $\Y$ induced by the sample
  $y^k$. Specifically,
\[
P_{y^k}(a) = \frac{\#\{ \mbox{occurrences of } a \mbox{ in } y^k\}}{k}
\]
\end{enumerate} 

\begin{lemma}
\label{lemma:1}
For any distribution $Q$ and for any sample $y^k$ (not neccessarily
from $Q$), the likelihood of $y^k$ drawn from $Q$ is given by
\[
P(y^k|Q) = 2^{-k(H(P_{y^k}) + \dkl(P_{y^k} || Q))}
\]
\end{lemma}
\begin{proof}
  \begin{align*}
    P(y^k|Q) 
&= \prod_{i=1}^k Q(x_i)\\ 
&= \prod_{a \in \Y} Q(a)^{nP_{y^k}(a)}\\ 
&= \prod_{a \in \Y} 2^{nP_{y^k}(a) \log Q(a)}\\ 
&= \prod_{a \in \Y} 2^{n(P_{y^k}(a) \log Q(a) - P_{y^k}(a) \log P_{y^k}(a) + P_{y^k}(a) \log P_{y^k}(a))}\\ 
&= 2^{k \sum_{a \in \Y} (-P_{y^k}(a) \log \frac{P_{y^k}(a)}{Q(a)} + P_{y^k}(a) \log P_{y^k}(a) )}\\ 
&= 2^{k(-\dkl(P_{y^k}||Q) - H(P_{y^k}))}\\
  \end{align*}
\end{proof}


\begin{collorary}
\label{col:min_dkl}
Given a set of distributions $\mathcal{Q} = \{Q_1, Q_2, \ldots,
Q_{M}\}$. For any sample $y^k$ drawn from any distribution, $Q_{i^*}
\in \mathcal{Q}$ maximizes the likelihood of $y^k$ among all other
distributions in $\mathcal{Q}$ if and only if $Q_{i^*}$ minimizes the
KL-divergence from $P_{y^k}$.
\[
 i^* = \arg\max_i \log P(y^k|Q_i) = \arg\min_i \dkl(P_{y^k}||Q_i)
\]
\end{collorary}
\begin{proof}
  Applying Lemma~\ref{lemma:1}, we have
  \begin{align*}
    P(y^k|Q_i) &= 2^{-n(H(P_{y^k}) + \dkl(P_{y^k} || Q_i))}\\
    \log P(y^k|Q_i) &= -n(H(P_{y^k}) + \dkl(P_{y^k} || Q_i))\\
    \arg\max_i \log P(y^k|Q_i) &= \arg\max_i - n\dkl(P_{y^k} || Q_i))\\ 
    &= \arg\min_i \dkl(P_{y^k} || Q_i))\\
  \end{align*}
\end{proof}


\section{Choice of $\mathcal{Q}$}

The next question is how should we select the prototypical
distribution $Q_j$. We will consider two natural choices. 

\subsection{Maximum Chernoff Information (MCI)}
The idea is to find $\mathcal{Q}$ such that the KL-divergence between
any two distributions $Q_i$ and $Q_j$ is maximized.

A simple solution to this is
\[
Q_i(j) = \delta_i(j)
\]
where $\delta_i(j) = \begin{cases} 1 &\mbox{ if } j = i\\ 0 &\mbox{ otherwise} \end{cases}$

When $\mathcal{Q}$ is MCI, for any distribution $P$, the minimizing
KL-divergence decision rule reduces to the majority decision rule.
\begin{align*}
\arg\min_i \dkl(P||Q_i) &= \arg\min_i -\sum_{j} P(j) \log Q_i(j)\\
 &= \arg\max_i P(i)\\
\end{align*}


\subsection{Maximum Likelihood Estimate (MLE)}

Given a training data $\D = \{(x_1,y_1,\ldots,(x_n,y_n)\}$ and the
label neighborhood of size $k$ for each $x_i$, $L_k(x)$, we want to
find $\mathcal{Q}$ that satisfies the following constaint. For each $y
\in \Y$, 
\[
\mbox{maximize } \sum_{(x,y) \in \D_y} \log P(L_k(x)|Q) \mbox{ subject to } \sum_{j} Q(j) = 1 
\]
where $\D_y = \{(x_i,y_i) | (x_i,y_i) \in \D \wedge y_i = y\}$

Using Lagrange multiplier,
\[
\nabla_{Q(i),\lambda} \left[ \sum_{(x,y) \in \D_y} \log P(L_k(x)|Q) + \lambda \sum_{j} Q(j) \right] = 0
\]

Let $P_x$ denote the empirical distribution over the labels induced by
$L_k(x)$. Using Lemma~\ref{lemma:1}, for each $i$,
\[
\frac{\partial}{\partial Q(i)} \sum_{(x,y) \in \D_y} \log P(L_k(x)|Q) + \lambda
(\sum_{j} Q(j)) = \sum_{(x,y) \in \D_y} k \frac{P_x(i)}{Q(i)} + \lambda = 0
\]
or
\[
Q(i) = \frac{k}{\lambda} \sum_{(x,y) \in \D_y} P_x(i)
\]
Since $\sum_{j} Q(j) = 1$, we have
\begin{align*}
\sum_{j} \frac{k}{\lambda} \sum_{(x,y) \in \D_y} P_x(j) &= 1\\
\frac{k}{\lambda} \sum_{j} \sum_{(x,y) \in \D_y} P_x(j) &= 1\\
\frac{k}{\lambda} \sum_{(x,y) \in \D_y} \sum_{j} P_x(j) &= 1\\
\frac{k}{\lambda} |\D_y| &= 1\\
\lambda &= |\D_y|k\\
\end{align*}
Plugging $\lambda$ back in,
\[
Q(i) = \frac{\sum_{(x,y) \in \D_y} P_x(i)}{|\D_y|}
\]

This shows that the MLE of $Q_i$ is simply the mean of the posterior
estimates of class $i$.


\section{Experimental Results}

\subsection{Synthetic dataset}

\subsection{Real-world dataset}

\section{Discussion}

Why does this work?

When does this work?

When does this not work?


\section{Conclusions}

We suggest a modification to k-NN algorithm that incorporates 


\bibliography{example_paper}
\bibliographystyle{icml2014}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
